#!/usr/bin/env python3
import os
import torch
import numpy as np
from pathlib import Path
from transformers import AutoConfig, AutoTokenizer
from dataclasses import dataclass
from copy import deepcopy

from modeling_intj import MultimodalQwen2Config, MultimodalQwen2ForCausalLM

# å¯¼å…¥LLaMA Factoryç›¸å…³æ¨¡å—
from llamafactory.model.loader import load_tokenizer
from llamafactory.hparams import ModelArguments, DataArguments
from llamafactory.data.mm_plugin import BasePlugin, register_mm_plugin, get_mm_plugin, PLUGINS
from llamafactory.data.collator import MultiModalDataCollatorForSeq2Seq
from llamafactory.data.template import register_template, get_template_and_fix_tokenizer
from llamafactory.extras.constants import IMAGE_PLACEHOLDER


# å®šä¹‰ä¸€ä¸ªç‚¹äº‘æ•°æ®ç±»æ¥æ–¹ä¾¿æµ‹è¯•
class PointCloudData:
    def __init__(self, patches, patch_coords):
        self.patches = patches
        self.patch_coords = patch_coords


def create_multimodal_qwen2_model(base_model_path, output_path):
    """åˆ›å»ºå¹¶ä¿å­˜MultimodalQwen2æ¨¡å‹"""
    # 1. è®¾ç½®å‚æ•°
    model_args = ModelArguments(
        model_name_or_path=base_model_path,
        add_special_tokens="<pointcloud>,<point_patch>,<layer_sep>,<row_sep>,</pointcloud>",
        resize_vocab=True
    )
    
    # 2. åŠ è½½tokenizerå¹¶æ·»åŠ ç‰¹æ®Štoken
    tokenizer_module = load_tokenizer(model_args)
    tokenizer = tokenizer_module["tokenizer"]
    
    # 3. åˆ›å»ºé…ç½®
    config = AutoConfig.from_pretrained(base_model_path)
    config_dict = config.to_dict()
    config_dict["architectures"] = ["MultimodalQwen2ForCausalLM"]
    config_dict["point_patch_size"] = 512  # ç‚¹äº‘patchå¤§å°
    
    # 4. åˆ›å»ºå¤šæ¨¡æ€é…ç½®å’Œæ¨¡å‹
    config = MultimodalQwen2Config.from_dict(config_dict)
    config.vocab_size = len(tokenizer)  # æ›´æ–°è¯è¡¨å¤§å°
    
    # 5. åˆå§‹åŒ–æ¨¡å‹
    model = MultimodalQwen2ForCausalLM(config)
    
    # 6. è°ƒæ•´embeddingså¤§å°ä»¥åŒ¹é…tokenizer
    model.resize_token_embeddings(len(tokenizer))
    
    # 7. ä¿å­˜æ¨¡å‹å’Œtokenizer
    os.makedirs(output_path, exist_ok=True)
    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    config.save_pretrained(output_path)
    
    return model, tokenizer, config


def register_qwen2_pointcloud_template():
    """æ³¨å†Œç‚¹äº‘æ¨¡æ¿"""
    # ç¡®ä¿ä½¿ç”¨æˆ‘ä»¬å·²ç»å®šä¹‰å¥½çš„plugin
    if "qwen2_pointcloud" in PLUGINS:
        class Qwen2PointCloudTemplate:
            def __init__(self, mm_plugin=None, system=None):
                self.mm_plugin = mm_plugin
                self.default_system = system
                
            def format_user(self, message):
                return f"USER: {message} ASSISTANT:"
                
            def format_assistant(self, message):
                return f" {message}"
                
            def format_system(self, message):
                return f"SYSTEM: {message}\n"
                
            def encode_multiturn(self, tokenizer, messages, system=None, tools=None):
                system = system or self.default_system
                result = ""
                if system:
                    result += self.format_system(system)
                for message in messages:
                    if message["role"] == "user":
                        result += self.format_user(message["content"])
                    else:
                        result += self.format_assistant(message["content"])
                return tokenizer.encode(result, add_special_tokens=True)
        
        # æ³¨å†Œæ¨¡æ¿
        plugin = get_mm_plugin(name="qwen2_pointcloud")
        return register_template(
            name="qwen2_pointcloud",
            template_class=Qwen2PointCloudTemplate,
            mm_plugin=plugin
        )
    else:
        raise ValueError("qwen2_pointcloud pluginæœªå®šä¹‰ï¼Œè¯·å…ˆå®šä¹‰plugin")


def test_model(model_path):
    """æµ‹è¯•æ¨¡å‹èƒ½å¦å¤„ç†ç‚¹äº‘è¾“å…¥"""
    print("\n===== æµ‹è¯•æ¨¡å‹åŸºæœ¬åŠŸèƒ½ =====")
    # 1. åŠ è½½æ¨¡å‹å’Œtokenizer
    model = MultimodalQwen2ForCausalLM.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # 2. åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    prompt = f"This is a point cloud: {IMAGE_PLACEHOLDER} Describe what you see."
    inputs = tokenizer(prompt, return_tensors="pt")
    print(f"è¾“å…¥ID: {inputs['input_ids'][0].tolist()}")
    print(f"æ¨¡æ‹Ÿæç¤º: {prompt}")
    print(f"tokenizeré•¿åº¦: {len(tokenizer)}")
    
    # 3. æ‰“å°ç‰¹æ®Štokençš„ID
    print(f"ç‰¹æ®Štoken ID:")
    special_tokens = ["<pointcloud>", "</pointcloud>", "<layer_sep>", "<row_sep>", "<point_patch>"]
    for token in special_tokens:
        token_id = tokenizer.convert_tokens_to_ids(token)
        print(f"  {token} -> {token_id}")
    
    # 4. åˆ›å»ºç‚¹äº‘æ•°æ® (100ä¸ªç‚¹ï¼Œæ¯ä¸ªç‚¹6ç»´ç‰¹å¾)
    num_points = 100
    point_features = 6
    point_cloud = torch.rand(num_points, point_features * 512)  # 512æ˜¯point_patch_size
    
    # 5. åˆ›å»ºpoint_patch_indices
    seq_length = inputs["input_ids"].shape[1]
    point_patch_id = tokenizer.convert_tokens_to_ids("<point_patch>")
    
    # åˆ›å»ºå…¨-1å¼ é‡
    point_indices = torch.full_like(inputs["input_ids"], -1, dtype=torch.long)
    
    # æ‰¾å‡º<point_patch>æ ‡è®°çš„ä½ç½®
    input_ids = inputs["input_ids"][0].tolist()
    patch_positions = [i for i, id in enumerate(input_ids) if id == point_patch_id]
    print(f"ç‚¹äº‘tokenä½ç½®: {patch_positions}")
    
    # ä¸ºæ¯ä¸ª<point_patch>ä½ç½®åˆ†é…ç‚¹äº‘ç´¢å¼•
    for idx, pos in enumerate(patch_positions):
        if idx < num_points:
            point_indices[0, pos] = idx
    
    # 6. æ¨¡å‹å‰å‘ä¼ æ’­
    try:
        outputs = model(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            point_patch_indices=point_indices,
            point_patches=point_cloud
        )
        print("âœ… æ¨¡å‹å‰å‘ä¼ æ’­æˆåŠŸ!")
        print(f"è¾“å‡ºlogitså½¢çŠ¶: {outputs.logits.shape}")
        print(f"è¾“å…¥åºåˆ—é•¿åº¦: {seq_length}")
        print(f"ç‚¹äº‘ç´¢å¼•å½¢çŠ¶: {point_indices.shape}")
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return False


def test_plugin_directly():
    """ç›´æ¥æµ‹è¯•pluginåŠŸèƒ½"""
    print("\n===== ç›´æ¥æµ‹è¯•PluginåŠŸèƒ½ =====")
    # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
    patches = np.random.rand(3, 10, 6).astype(np.float32)  # 3ä¸ªpatchï¼Œæ¯ä¸ªæœ‰10ä¸ªç‚¹ï¼Œ6ä¸ªç‰¹å¾
    patch_coords = np.array([[0,0,0], [0,1,0], [1,0,0]])   # 3ä¸ªpatchçš„åæ ‡
    pointcloud_data = PointCloudData(patches, patch_coords)
    
    # 2. åŠ è½½tokenizer
    model_args = ModelArguments(
        model_name_or_path=OUTPUT_PATH,
        add_special_tokens="<pointcloud>,<point_patch>,<layer_sep>,<row_sep>,</pointcloud>",
    )
    tokenizer_module = load_tokenizer(model_args)
    tokenizer = tokenizer_module["tokenizer"]
    processor = tokenizer_module["processor"]
    
    # 3. è·å–plugin
    pointcloud_plugin = get_mm_plugin(name="qwen2_pointcloud")
    
    # 4. æµ‹è¯•process_messages
    test_messages = [
        {"role": "user", "content": f"Here is a pointcloud: {IMAGE_PLACEHOLDER} Describe it."},
        {"role": "assistant", "content": "I see points in 3D space."},
    ]
    
    processed_messages = pointcloud_plugin.process_messages(
        deepcopy(test_messages), [pointcloud_data], [], [], processor
    )
    
    print("åŸå§‹æ¶ˆæ¯:")
    print(test_messages[0]["content"])
    print("\nå¤„ç†åæ¶ˆæ¯:")
    print(processed_messages[0]["content"])
    
    # 5. æµ‹è¯•tokenization
    tokens = tokenizer.tokenize(processed_messages[0]["content"])
    print("\nåˆ†è¯ç»“æœ:")
    print(tokens)
    
    # 6. æµ‹è¯•_regularize_images
    reg_results = pointcloud_plugin._regularize_images([pointcloud_data])
    print("\n_regularize_imagesç»“æœ:")
    print(f"patchæ•°é‡: {len(reg_results['point_patches'][0])}")
    
    # 7. æµ‹è¯•get_mm_inputs
    # æ¨¡æ‹Ÿbatch_ids
    token_ids = tokenizer.encode(processed_messages[0]["content"])
    point_patch_id = tokenizer.convert_tokens_to_ids("<point_patch>")
    
    mm_inputs = pointcloud_plugin.get_mm_inputs(
        [pointcloud_data], [], [], [1], [], [], [token_ids], processor
    )
    
    print("\nget_mm_inputsç»“æœ:")
    print(f"point_patch_indiceså½¢çŠ¶: {mm_inputs['point_patch_indices'].shape}")
    if 'point_patches' in mm_inputs:
        print(f"point_patcheså½¢çŠ¶: {mm_inputs['point_patches'].shape}")
    
    return True


def test_with_llamafactory_collator(model_path):
    """ä½¿ç”¨LLaMA Factoryçš„collatoræµ‹è¯•"""
    print("\n===== æµ‹è¯•ä¸LLaMA Factoryé›†æˆ =====")
    
    # 0. æ³¨å†Œæ¨¡æ¿
    try:
        template_name = register_qwen2_pointcloud_template()
        print(f"âœ… æˆåŠŸæ³¨å†Œæ¨¡æ¿: {template_name}")
    except Exception as e:
        print(f"âŒ æ¨¡æ¿æ³¨å†Œå¤±è´¥: {e}")
        return False
    
    # 1. å‡†å¤‡å‚æ•°
    model_args = ModelArguments(model_name_or_path=model_path)
    data_args = DataArguments(template=template_name)
    
    # 2. åŠ è½½tokenizerå’Œæ¨¡æ¿
    tokenizer_module = load_tokenizer(model_args)
    tokenizer = tokenizer_module["tokenizer"]
    processor = tokenizer_module["processor"]
    
    try:
        template = get_template_and_fix_tokenizer(tokenizer, data_args)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡æ¿")
    except Exception as e:
        print(f"âŒ æ¨¡æ¿åŠ è½½å¤±è´¥: {e}")
        return False
    
    # 3. åˆ›å»ºcollator
    try:
        data_collator = MultiModalDataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            model=None,
            template=template,
            processor=processor,
            padding="longest",
            max_length=512,
            pad_to_multiple_of=8
        )
        print(f"âœ… æˆåŠŸåˆ›å»ºcollator")
    except Exception as e:
        print(f"âŒ collatoråˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # 4. åˆ›å»ºæµ‹è¯•æ•°æ®
    # åˆ›å»ºç‚¹äº‘æ•°æ®
    pointcloud_patches = np.random.randn(3, 10, 6).astype(np.float32)  # 3ä¸ªpatchï¼Œæ¯ä¸ªæœ‰10ä¸ªç‚¹
    pointcloud_coords = np.array([[0,0,0], [0,1,0], [1,0,0]]).astype(np.float32)  # 3ä¸ªpatchåæ ‡
    pointcloud_data = PointCloudData(pointcloud_patches, pointcloud_coords)
    
    # ç»„è£…ç‰¹å¾
    features = [
        {
            "input_ids": tokenizer(f"USER: Describe this point cloud: {IMAGE_PLACEHOLDER} ASSISTANT:").input_ids,
            "attention_mask": [1] * len(tokenizer(f"USER: Describe this point cloud: {IMAGE_PLACEHOLDER} ASSISTANT:").input_ids),
            "labels": tokenizer(" This is a set of points in 3D space.").input_ids,
            "images": [pointcloud_data],  # ä½¿ç”¨imageså­—æ®µä¼ é€’ç‚¹äº‘æ•°æ®
            "videos": [],
            "audios": []
        }
    ]
    
    # 5. æµ‹è¯•collator
    try:
        batch = data_collator(features)
        print("âœ… Collatorå¤„ç†æˆåŠŸ!")
        print(f"æ‰¹å¤„ç†é”®: {batch.keys()}")
        
        if "point_patch_indices" in batch:
            print(f"ç‚¹äº‘ç´¢å¼•å½¢çŠ¶: {batch['point_patch_indices'].shape}")
            
        if "point_patches" in batch:
            print(f"ç‚¹äº‘ç‰¹å¾å½¢çŠ¶: {batch['point_patches'].shape}")
            return True
        else:
            print("âš ï¸ è­¦å‘Š: ç¼ºå°‘ç‚¹äº‘ç‰¹å¾!")
            return False
    except Exception as e:
        print(f"âŒ Collatorå¤„ç†å¤±è´¥: {e}")
        print("é”™è¯¯è¯¦æƒ…:", str(e))
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    BASE_MODEL_PATH = "/pscratch/sd/c/cheryunl/qwen2_0.5b_cache"  # æˆ–ä½ æœ¬åœ°çš„Qwen2æ¨¡å‹è·¯å¾„
    OUTPUT_PATH = "./multimodal_qwen2_model" 
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºMultimodalQwen2æ¨¡å‹...")
    model, tokenizer, config = create_multimodal_qwen2_model(BASE_MODEL_PATH, OUTPUT_PATH)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {OUTPUT_PATH}")
    
    # æµ‹è¯•æ¨¡å‹
    print("\næµ‹è¯•æ¨¡å‹å¤„ç†èƒ½åŠ›...")
    test_model_success = test_model(OUTPUT_PATH)
    
    # ç›´æ¥æµ‹è¯•pluginåŠŸèƒ½
    print("\nç›´æ¥æµ‹è¯•Plugin...")
    test_plugin_success = test_plugin_directly()
    
    # æµ‹è¯•LLaMA Factoryé›†æˆ
    if test_model_success and test_plugin_success:
        print("\næµ‹è¯•ä¸LLaMA Factoryé›†æˆ...")
        collator_success = test_with_llamafactory_collator(OUTPUT_PATH)
        
        if collator_success:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! ç‚¹äº‘æ¨¡å‹å’ŒPluginè¿è¡Œæ­£å¸¸ã€‚")
        else:
            print("\nâš ï¸ LLaMA Factoryé›†æˆæµ‹è¯•å¤±è´¥ï¼Œä½†åŸºæœ¬åŠŸèƒ½æ­£å¸¸ã€‚")
    else:
        print("\nâŒ åŸºæœ¬æµ‹è¯•å¤±è´¥ï¼Œè¯·ä¿®å¤åŸºç¡€é—®é¢˜ã€‚")