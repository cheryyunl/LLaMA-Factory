# ä» Qwen2.5-7B-Instruct åˆå§‹åŒ–å¤šæ¨¡æ€æ¨¡å‹

import torch
import os
import sys
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig

# æ·»åŠ è·¯å¾„
sys.path.append('/code/LLaMA-Factory')
from modeling_lemon import MultimodalQwen2Config, MultimodalQwen2ForCausalLM

def create_multimodal_from_qwen25():
    """ä» Qwen2.5-7B-Instruct åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹"""
    
    print("=== æ­¥éª¤1: åŠ è½½ Qwen2.5-7B-Instruct ===")
    qwen25_path = "/code/Qwen2.5-7B-Instruct"
    
    # åŠ è½½åŸå§‹æ¨¡å‹å’Œé…ç½®
    original_model = AutoModelForCausalLM.from_pretrained(qwen25_path)
    tokenizer = AutoTokenizer.from_pretrained(qwen25_path)
    
    print(f"âœ… åŸå§‹æ¨¡å‹åŠ è½½å®Œæˆ: {type(original_model).__name__}")
    
    # æ£€æŸ¥é¢„ç•™ç©ºé—´
    original_embed_size = original_model.get_input_embeddings().weight.shape[0]  # 152064
    tokenizer_size = len(tokenizer)  # 151643
    reserved_space = original_embed_size - tokenizer_size  # 421ä¸ªé¢„ç•™ä½ç½®
    
    print(f"åŸå§‹embeddingå¤§å°: {original_embed_size}")
    print(f"Tokenizerå¤§å°: {tokenizer_size}")
    print(f"é¢„ç•™ç©ºé—´: {reserved_space} ä¸ªä½ç½®")
    
    # ğŸ”§ æ‰‹åŠ¨åˆ†é…ç‰¹æ®Štoken IDåˆ°é¢„ç•™ç©ºé—´
    special_tokens = ["<pointcloud>", "</pointcloud>", "<point_patch>", "<row_sep>", "<layer_sep>"]
    
    if len(special_tokens) <= reserved_space:
        print(f"âœ… é¢„ç•™ç©ºé—´è¶³å¤Ÿï¼Œåˆ†é…ç‰¹æ®Štokenåˆ°ä½ç½® {tokenizer_size}-{tokenizer_size + len(special_tokens) - 1}")
        
        # æ‰‹åŠ¨æ·»åŠ tokenåˆ°é¢„ç•™ä½ç½®
        for i, token in enumerate(special_tokens):
            token_id = tokenizer_size + i
            tokenizer.add_tokens([token])
            print(f"  {token} -> {token_id}")
            
        print(f"æ–°çš„tokenizerå¤§å°: {len(tokenizer)}")
    else:
        print(f"âŒ é¢„ç•™ç©ºé—´ä¸å¤Ÿï¼éœ€è¦ {len(special_tokens)}ï¼Œåªæœ‰ {reserved_space}")
        return None, None
    
    print("\n=== æ­¥éª¤2: åˆ›å»ºå¤šæ¨¡æ€é…ç½® ===")
    # ğŸ”§ å…³é”®ï¼šä¿æŒåŸå§‹çš„vocab_sizeä¸å˜
    multimodal_config = MultimodalQwen2Config.from_pretrained(qwen25_path)
    multimodal_config.point_patch_size = 512
    # ä¸ä¿®æ”¹vocab_sizeï¼Œä¿æŒ152064
    
    print(f"âœ… å¤šæ¨¡æ€é…ç½®åˆ›å»ºå®Œæˆ")
    print(f"ä¿æŒåŸå§‹vocab_size: {multimodal_config.vocab_size}")
    
    print("\n=== æ­¥éª¤3: åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹ ===")
    # åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹
    multimodal_model = MultimodalQwen2ForCausalLM(multimodal_config)

    print(f"æ–°æ¨¡å‹embeddingså¤§å°: {multimodal_model.get_input_embeddings().weight.shape}")
    print(f"åŸå§‹æ¨¡å‹embeddingså¤§å°: {original_model.get_input_embeddings().weight.shape}")

    print("\n=== æ­¥éª¤4: æƒé‡è¿ç§» ===")
    # ğŸ”§ å…ˆåŠ è½½åŸå§‹æƒé‡
    original_state_dict = original_model.state_dict()
    multimodal_model.load_state_dict(original_state_dict, strict=False)

    print(f"âœ… æƒé‡è¿ç§»å®Œæˆï¼Œåˆ©ç”¨é¢„ç•™ç©ºé—´")

    # ğŸ”§ æ‰‹åŠ¨åˆå§‹åŒ–ç‰¹æ®Štokençš„embeddingå’Œlm_head
    print("æ‰‹åŠ¨åˆå§‹åŒ–ç‰¹æ®Štokensçš„embeddingå’Œlm_head...")

    with torch.no_grad():
        # è·å–ç°æœ‰æœ‰æ•ˆtokençš„æƒé‡ï¼ˆæ’é™¤ç‰¹æ®ŠtokenåŒºåŸŸï¼‰
        valid_token_count = tokenizer_size - len(special_tokens)  # 151643 - 5 = 151638
        
        valid_embeddings = multimodal_model.get_input_embeddings().weight[:valid_token_count]
        valid_lm_head = multimodal_model.lm_head.weight[:valid_token_count]
        
        # è®¡ç®—ç°æœ‰tokençš„ç»Ÿè®¡ä¿¡æ¯
        embed_mean = valid_embeddings.mean(dim=0)
        embed_std = valid_embeddings.std().item()
        
        lm_head_mean = valid_lm_head.mean(dim=0)  
        lm_head_std = valid_lm_head.std().item()
        
        print(f"ç°æœ‰tokenç»Ÿè®¡:")
        print(f"  embedding: å‡å€¼norm={embed_mean.norm().item():.4f}, std={embed_std:.6f}")
        print(f"  lm_head:   å‡å€¼norm={lm_head_mean.norm().item():.4f}, std={lm_head_std:.6f}")
        
        print(f"\nåˆå§‹åŒ– {len(special_tokens)} ä¸ªç‰¹æ®Štokens:")
        
        for token in special_tokens:
            token_id = tokenizer.convert_tokens_to_ids(token)
            
            # è®°å½•åˆå§‹åŒ–å‰çš„å€¼
            old_embed_norm = multimodal_model.get_input_embeddings().weight[token_id].norm().item()
            old_lm_head_norm = multimodal_model.lm_head.weight[token_id].norm().item()
            
            # åˆå§‹åŒ–embeddingï¼šä½¿ç”¨å‡å€¼ + å°çš„éšæœºæ‰°åŠ¨
            multimodal_model.get_input_embeddings().weight[token_id] = (
                embed_mean + torch.randn_like(embed_mean) * embed_std * 0.1
            )
            
            # åˆå§‹åŒ–lm_headï¼šä½¿ç”¨å‡å€¼ + å°çš„éšæœºæ‰°åŠ¨  
            multimodal_model.lm_head.weight[token_id] = (
                lm_head_mean + torch.randn_like(lm_head_mean) * lm_head_std * 0.1
            )
            
            # è®°å½•åˆå§‹åŒ–åçš„å€¼
            new_embed_norm = multimodal_model.get_input_embeddings().weight[token_id].norm().item()
            new_lm_head_norm = multimodal_model.lm_head.weight[token_id].norm().item()
            
            print(f"  {token} (ID:{token_id}):")
            print(f"    embedding: {old_embed_norm:.6f} -> {new_embed_norm:.4f}")
            print(f"    lm_head:   {old_lm_head_norm:.6f} -> {new_lm_head_norm:.4f}")

    # æœ€ç»ˆéªŒè¯
    print(f"\n=== éªŒè¯ç‰¹æ®Štokenåˆå§‹åŒ–ç»“æœ ===")
    all_good = True

    for token in special_tokens:
        token_id = tokenizer.convert_tokens_to_ids(token)
        embed_norm = multimodal_model.get_input_embeddings().weight[token_id].norm().item()
        lm_head_norm = multimodal_model.lm_head.weight[token_id].norm().item()
        
        # æ£€æŸ¥æ˜¯å¦åˆç†ï¼ˆä¸æ˜¯0ï¼Œä¸æ˜¯å¼‚å¸¸å¤§å€¼ï¼‰
        embed_ok = 0.01 < embed_norm < 1.0
        lm_head_ok = 0.01 < lm_head_norm < 1.0
        
        status = "âœ…" if (embed_ok and lm_head_ok) else "âŒ"
        print(f"  {status} {token} (ID:{token_id}): embedding={embed_norm:.4f}, lm_head={lm_head_norm:.4f}")
        
        if not (embed_ok and lm_head_ok):
            all_good = False

    if all_good:
        print("âœ… æ‰€æœ‰ç‰¹æ®Štokenåˆå§‹åŒ–æˆåŠŸï¼")
    else:
        print("âŒ éƒ¨åˆ†ç‰¹æ®Štokenåˆå§‹åŒ–å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥")

    print("âœ… ç‰¹æ®Štokenæ‰‹åŠ¨åˆå§‹åŒ–å®Œæˆ")
    
    print("\n=== æ­¥éª¤5: ä¿å­˜å¤šæ¨¡æ€æ¨¡å‹ ===")
    save_path = "/code/MultimodalQwen2.5-7B-Instruct"
    os.makedirs(save_path, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹å’Œé…ç½®
    multimodal_model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    
    # å¤åˆ¶ modeling_lemon.py åˆ°æ¨¡å‹ç›®å½•
    import shutil
    shutil.copy('/code/LLaMA-Factory/modeling_lemon.py', os.path.join(save_path, 'modeling_lemon.py'))
    
    # ä¿®æ”¹ config.json æ·»åŠ  auto_map
    import json
    config_path = os.path.join(save_path, 'config.json')
    with open(config_path, 'r') as f:
        config_json = json.load(f)
    
    config_json.update({
        "model_type": "multimodal_qwen2",
        "architectures": ["MultimodalQwen2ForCausalLM"],
        "auto_map": {
            "AutoConfig": "modeling_lemon.MultimodalQwen2Config",
            "AutoModelForCausalLM": "modeling_lemon.MultimodalQwen2ForCausalLM"
        }
    })
    
    with open(config_path, 'w') as f:
        json.dump(config_json, f, indent=2)
    
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    print(f"âœ… é…ç½®æ–‡ä»¶å·²æ›´æ–°")
    
    return save_path, multimodal_model

def test_multimodal_model(model_path):
    """æµ‹è¯•å¤šæ¨¡æ€æ¨¡å‹"""
    
    print(f"\n=== æµ‹è¯•æ¨¡å‹: {model_path} ===")
    
    # åŠ è½½æ¨¡å‹
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {type(model).__name__}")
    print(f"âœ… æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # æ£€æŸ¥å¤šæ¨¡æ€ç»„ä»¶
    has_point_patch = hasattr(model.model, 'embed_point_patch')
    print(f"âœ… åŒ…å«ç‚¹äº‘åµŒå…¥å±‚: {has_point_patch}")
    
    if has_point_patch:
        point_patch_shape = model.model.embed_point_patch.weight.shape
        print(f"âœ… ç‚¹äº‘åµŒå…¥å±‚å½¢çŠ¶: {point_patch_shape}")
    
    print("\n--- æµ‹è¯•1: çº¯æ–‡æœ¬ Forward ---")
    text = "Hello, how are you today?"
    inputs = tokenizer(text, return_tensors="pt")
    
    try:
        with torch.no_grad():
            outputs = model(**inputs)
        print(f"âœ… æ–‡æœ¬ForwardæˆåŠŸ, logitså½¢çŠ¶: {outputs.logits.shape}")
    except Exception as e:
        print(f"âŒ æ–‡æœ¬Forwardå¤±è´¥: {e}")
        return False
    
    print("\n--- æµ‹è¯•2: çº¯æ–‡æœ¬ Generate ---")
    try:
        with torch.no_grad():
            generated = model.generate(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_new_tokens=20,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
        print(f"âœ… æ–‡æœ¬GenerateæˆåŠŸ")
        print(f"   è¾“å…¥: {text}")
        print(f"   è¾“å‡º: {generated_text}")
    except Exception as e:
        print(f"âŒ æ–‡æœ¬Generateå¤±è´¥: {e}")
        return False
    
    print("\n--- æµ‹è¯•3: å¤šæ¨¡æ€ Forward ---")
    if has_point_patch:
        batch_size, seq_len = 1, 10
        n_patches = 3
        
        # æ¨¡æ‹Ÿå¤šæ¨¡æ€è¾“å…¥
        dummy_input_ids = torch.randint(1, 1000, (batch_size, seq_len))  # é¿å…ç‰¹æ®Štoken
        dummy_attention_mask = torch.ones_like(dummy_input_ids)
        dummy_point_patches = torch.randn(n_patches, 512 * 6)
        dummy_point_patch_indices = torch.full((batch_size, seq_len), -1, dtype=torch.long)
        dummy_point_patch_indices[:, :3] = torch.tensor([0, 1, 2])
        
        try:
            with torch.no_grad():
                outputs = model(
                    input_ids=dummy_input_ids,
                    attention_mask=dummy_attention_mask,
                    point_patches=dummy_point_patches,
                    point_patch_indices=dummy_point_patch_indices
                )
            print(f"âœ… å¤šæ¨¡æ€ForwardæˆåŠŸ, logitså½¢çŠ¶: {outputs.logits.shape}")
        except Exception as e:
            print(f"âŒ å¤šæ¨¡æ€Forwardå¤±è´¥: {e}")
            return False
        
        print("\n--- æµ‹è¯•4: å¤šæ¨¡æ€ Generate ---")
        try:
            with torch.no_grad():
                generated = model.generate(
                    input_ids=dummy_input_ids,
                    attention_mask=dummy_attention_mask,
                    point_patches=dummy_point_patches,
                    point_patch_indices=dummy_point_patch_indices,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id
                )
            print(f"âœ… å¤šæ¨¡æ€GenerateæˆåŠŸ, è¾“å‡ºå½¢çŠ¶: {generated.shape}")
        except Exception as e:
            print(f"âŒ å¤šæ¨¡æ€Generateå¤±è´¥: {e}")
            return False
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å·¥ä½œæ­£å¸¸")
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ä» Qwen2.5-7B-Instruct åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹...")
    
    try:
        # åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹
        model_path, model = create_multimodal_from_qwen25()
        
        # æµ‹è¯•æ¨¡å‹
        success = test_multimodal_model(model_path)
        
        if success:
            print(f"\nğŸ‰ æˆåŠŸï¼å¤šæ¨¡æ€æ¨¡å‹å·²åˆ›å»ºå¹¶æµ‹è¯•é€šè¿‡")
            print(f"ğŸ“‚ æ¨¡å‹ä¿å­˜ä½ç½®: {model_path}")
            print(f"ğŸš€ å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
            
            print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
            print(f"from transformers import AutoModelForCausalLM, AutoTokenizer")
            print(f"model = AutoModelForCausalLM.from_pretrained('{model_path}', trust_remote_code=True)")
            print(f"tokenizer = AutoTokenizer.from_pretrained('{model_path}')")
        else:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
            
    except Exception as e:
        print(f"\nâŒ åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()