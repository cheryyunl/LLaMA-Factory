#!/usr/bin/env python3
import os
import json
import numpy as np
import argparse
from tqdm import tqdm
import multiprocessing as mp
import random
from collections import defaultdict
import numbers

def dynamic_axis_partition(points, scene_range, max_splits_per_axis=5, target_points=512):
    """
    Point cloud partitioning algorithm with strict constraints:
    - Maximum 5 splits per axis
    - Fixed 512 points per patch
    - Maximum 125 patches (5*5*5)
    """
    patches = []
    patch_coords = []

    # Z-axis partitioning - requires 512*4*4 points
    z_target = target_points * max_splits_per_axis * max_splits_per_axis  # 512*4*4
    z_coords = points[:, 2]
    z_splits, _ = calculate_splits(z_coords, scene_range[2], z_target, max_splits_per_axis)
    
    for z_idx in range(len(z_splits)-1):
        z_min, z_max = z_splits[z_idx], z_splits[z_idx+1]
        z_mask = (points[:, 2] >= z_min) & (points[:, 2] < z_max)
        z_layer = points[z_mask]
        if len(z_layer) == 0:
            continue

        # Y-axis partitioning - requires 512*4 points
        y_target = target_points * max_splits_per_axis  # 512*4
        y_coords = z_layer[:, 1]
        y_splits, _ = calculate_splits(y_coords, scene_range[1], y_target, max_splits_per_axis)

        for y_idx in range(len(y_splits)-1):
            y_min, y_max = y_splits[y_idx], y_splits[y_idx+1]
            y_mask = (z_layer[:, 1] >= y_min) & (z_layer[:, 1] < y_max)
            y_row = z_layer[y_mask]
            if len(y_row) == 0:
                continue

            # X-axis partitioning - requires 512 points
            x_coords = y_row[:, 0]
            x_splits, _ = calculate_splits(x_coords, scene_range[0], target_points, max_splits_per_axis)

            for x_idx in range(len(x_splits)-1):
                x_min, x_max = x_splits[x_idx], x_splits[x_idx+1]
                x_mask = (y_row[:, 0] >= x_min) & (y_row[:, 0] < x_max)
                patch = y_row[x_mask]
                
                # Maintain exactly 512 points
                processed_patch = adjust_points(patch, target_points)
                patch_coords.append((z_idx, y_idx, x_idx))
                patches.append(processed_patch)

    return patches, patch_coords


def calculate_splits(axis_coords, axis_range, target_points_per_split, max_splits):
    """Calculate split points based on point distribution"""
    sorted_coords = np.sort(axis_coords)
    total_points = len(sorted_coords)
    
    if total_points == 0:
        return [axis_range[0], axis_range[1]], 0
        
    # Split based on point distribution regardless of total points
    # Each partition should have at least target_points_per_split points
    required_splits = min(max_splits, max(1, total_points // target_points_per_split))
    
    # Determine split points based on cumulative point count ratio
    splits = [axis_range[0]]
    points_per_split = total_points / required_splits
    
    for i in range(1, required_splits):
        target_index = int(i * points_per_split)
        splits.append(sorted_coords[target_index])
    
    splits.append(axis_range[1])
    splits = list(np.unique(splits))
    
    return splits, len(splits)-1

def adjust_points(patch, target_points):
    """Strictly align the number of points to 512"""
    if len(patch) == 0:
        return np.zeros((target_points, patch.shape[1]))
    elif len(patch) > target_points:
        return fps_sampling(patch, target_points)
    else:
        repeat_times = (target_points // len(patch)) + 1
        return np.tile(patch, (repeat_times, 1))[:target_points]

def fps_sampling(points, n_samples, num_candidates=10):
    """Improved fast FPS sampling (fixed sampling when remaining < num_candidates)"""
    if len(points) <= n_samples:
        return points
    
    indices = [np.random.randint(len(points))]
    
    for _ in range(1, n_samples):
        # æ‰¾å‡ºå‰©ä½™çš„ç‚¹ç´¢å¼•
        remaining = np.where(~np.isin(np.arange(len(points)), indices))[0]
        if len(remaining) == 0:
            break
        
        # å¦‚æœå‰©ä½™ç‚¹å°‘äºnum_candidatesï¼Œåˆ™å–æ‰€æœ‰å‰©ä½™ç‚¹ï¼›å¦åˆ™éšæœºæŠ½num_candidates
        k = min(len(remaining), num_candidates)
        candidates = np.random.choice(remaining, k, replace=False)
        
        # è®¡ç®—æ¯ä¸ªå€™é€‰ç‚¹åˆ°å·²é€‰ç‚¹é›†ä¸­æœ€å°è·ç¦»
        dists = np.min(
            np.linalg.norm(points[candidates][:, None] - points[indices], axis=2),
            axis=1
        )
        # é€‰å–æœ€è¿œçš„ä¸€ä¸ª
        next_idx = candidates[np.argmax(dists)]
        indices.append(next_idx)
    
    return points[indices]
   
# é…ç½®
PROMPTS = [
    "<image>What is this object?",
    "<image>This is a point cloud. What is this object?",
    "<image>Identify this 3D object.",
    "<image>Looking at this point cloud, what object does it represent?",
    "<image>Please classify this 3D point cloud."
]

# ç³»ç»ŸæŒ‡ä»¤å¸¸é‡
SYSTEM_PROMPT = (
    "You are an AI assistant specialized in understanding 3D point cloud data "
    "with 6 dimensions: coordinates (x, y, z) and colors (r, g, b). "
)

def process_pointcloud_to_patches(xyz, rgb, normalize=None):
    """å°†ç‚¹äº‘å¤„ç†ä¸ºpatcheså’Œcoordinates"""
    # ç»„åˆxyzå’Œrgbä¸ºä¸€ä¸ª6ç»´æ•°ç»„
    arr = np.concatenate([xyz, rgb], axis=1)
    
    # ç»Ÿè®¡åŸå§‹èŒƒå›´
    x_min, x_max = xyz[:, 0].min(), xyz[:, 0].max()
    y_min, y_max = xyz[:, 1].min(), xyz[:, 1].max()
    z_min, z_max = xyz[:, 2].min(), xyz[:, 2].max()
    r_min, r_max = rgb[:, 0].min(), rgb[:, 0].max()
    g_min, g_max = rgb[:, 1].min(), rgb[:, 1].max()
    b_min, b_max = rgb[:, 2].min(), rgb[:, 2].max()
    
    # å½’ä¸€åŒ–å¤„ç†
    if normalize == 'xyz' or normalize == 'all':
        # å½’ä¸€åŒ–ç©ºé—´åæ ‡åˆ°[-1, 1]
        x_center = (x_min + x_max) / 2
        y_center = (y_min + y_max) / 2
        z_center = (z_min + z_max) / 2
        
        x_scale = max(abs(x_max - x_center), abs(x_min - x_center)) * 1.05  # ç•™ä¸€ç‚¹ä½™é‡
        y_scale = max(abs(y_max - y_center), abs(y_min - y_center)) * 1.05
        z_scale = max(abs(z_max - z_center), abs(z_min - z_center)) * 1.05
        
        max_scale = max(x_scale, y_scale, z_scale)
        
        xyz[:, 0] = (xyz[:, 0] - x_center) / max_scale
        xyz[:, 1] = (xyz[:, 1] - y_center) / max_scale
        xyz[:, 2] = (xyz[:, 2] - z_center) / max_scale
        
        # æ›´æ–°arr
        arr[:, :3] = xyz
        
    if normalize == 'rgb' or normalize == 'all':
        # å½’ä¸€åŒ–é¢œè‰²åˆ°[0, 1]
        if r_max > 1.0 or g_max > 1.0 or b_max > 1.0:  # æ¨æµ‹æ˜¯0-255èŒƒå›´
            rgb = rgb / 255.0
            arr[:, 3:] = rgb
    
    # è¿”å›åæ ‡èŒƒå›´ä¿¡æ¯
    ranges = {
        'x': (x_min, x_max),
        'y': (y_min, y_max),
        'z': (z_min, z_max),
        'r': (r_min, r_max),
        'g': (g_min, g_max),
        'b': (b_min, b_max)
    }
    
    # è®¡ç®—åœºæ™¯èŒƒå›´
    scene_range = [
        (xyz[:, 0].min(), xyz[:, 0].max()),
        (xyz[:, 1].min(), xyz[:, 1].max()),
        (xyz[:, 2].min(), xyz[:, 2].max())
    ]
    
    # åˆ†å‰²ç‚¹äº‘
    patches, patch_coords = dynamic_axis_partition(arr, scene_range)
    return patches, patch_coords, ranges

def to_python_type(obj):
    """å°†numpyç±»å‹è½¬æ¢ä¸ºPythonæ ‡å‡†ç±»å‹"""
    if isinstance(obj, dict):
        return {k: to_python_type(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [to_python_type(v) for v in obj]
    elif isinstance(obj, numbers.Number):
        return obj.item() if hasattr(obj, 'item') else obj
    else:
        return obj

def process_one_npy_file(args):
    """å¤„ç†å•ä¸ªnpyæ–‡ä»¶"""
    npy_path, output_dir, use_diverse_prompts, normalize = args
    
    # ç”Ÿæˆç›¸å¯¹è·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
    relpath = os.path.relpath(npy_path, start=os.path.dirname(output_dir))
    base_name = os.path.basename(npy_path).replace('.npy', '')
    
    # åˆ›å»ºnpzä¿å­˜ç›®å½• (ä¿æŒåŸç›®å½•ç»“æ„)
    npz_dir = os.path.join(output_dir, 'patches', os.path.dirname(relpath))
    os.makedirs(npz_dir, exist_ok=True)
    
    # ç”Ÿæˆnpzæ–‡ä»¶è·¯å¾„
    npz_path = os.path.join(npz_dir, f"{base_name}.npz")
    
    # æ£€æŸ¥npzæ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è·³è¿‡å¤„ç†
    if os.path.exists(npz_path):
        return {
            'status': 'skipped',
            'npy_path': npy_path,
            'npz_path': npz_path
        }
    
    try:
        # åŠ è½½npyæ–‡ä»¶
        data = np.load(npy_path, allow_pickle=True).item()
        
        # æå–xyz, rgbå’Œtextæ•°æ®
        xyz = data['xyz']
        rgb = data['rgb']
        
        # è·å–label (è½¬ä¸ºå°å†™)
        if isinstance(data['text'], list) and len(data['text']) > 0:
            label = data['text'][0].lower()
        else:
            label = str(data['text']).lower()
        
        # å¤„ç†ç‚¹äº‘ä¸ºpatcheså’Œpatch_coords
        patches, patch_coords, ranges = process_pointcloud_to_patches(xyz, rgb, normalize=normalize)
        
        # æ£€æŸ¥patchesæ˜¯å¦ä¸ºç©º
        if len(patches) == 0:
            return {
                'status': 'failed',
                'npy_path': npy_path,
                'reason': 'no_patches'
            }
        
        # ä¿å­˜ä¸ºnpz
        np.savez(npz_path, patches=patches, patch_coords=patch_coords)
        
        return {
            'status': 'success',
            'npy_path': npy_path,
            'npz_path': npz_path,
            'label': label,
            'num_patches': len(patches),
            'ranges': ranges
        }
        
    except Exception as e:
        return {
            'status': 'error',
            'npy_path': npy_path,
            'error': str(e)
        }

def find_all_npy_files(root_dir):
    """é€’å½’æŸ¥æ‰¾æ‰€æœ‰npyæ–‡ä»¶"""
    npy_files = []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith('.npy'):
                npy_files.append(os.path.join(root, file))
    return npy_files

def process_objaverse_lvis(root_dir, output_dir, n_proc=None, use_diverse_prompts=True, normalize=None):
    """å¤„ç†objaverse_lvisç›®å½•ä¸‹çš„æ‰€æœ‰npyæ–‡ä»¶"""
    if n_proc is None:
        n_proc = max(1, mp.cpu_count() - 1)  # é»˜è®¤ä½¿ç”¨n-1ä¸ªCPU
    
    # æ‰¾åˆ°æ‰€æœ‰çš„npyæ–‡ä»¶
    print(f"Scanning {root_dir} for npy files...")
    all_npy_files = find_all_npy_files(root_dir)
    
    if not all_npy_files:
        print(f"No npy files found in {root_dir}")
        return 0
        
    print(f"Found {len(all_npy_files)} npy files to process")
    
    # å‡†å¤‡ä»»åŠ¡å‚æ•°
    tasks = [(npy_file, output_dir, use_diverse_prompts, normalize) for npy_file in all_npy_files]
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    jsonl_path = os.path.join(output_dir, "objaverse_lvis.jsonl")
    
    # å¤šè¿›ç¨‹å¤„ç†
    print(f"Processing with {n_proc} workers...")
    with mp.Pool(n_proc) as pool:
        results = list(tqdm(pool.imap(process_one_npy_file, tasks), total=len(tasks), desc="Processing npy files"))
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'success': 0,
        'skipped': 0,
        'failed': 0,
        'error': 0,
        'total_patches': 0
    }
    
    # èŒƒå›´ç»Ÿè®¡
    ranges_stats = defaultdict(lambda: {'min': float('inf'), 'max': float('-inf')})
    
    # å†™å…¥jsonlæ–‡ä»¶
    with open(jsonl_path, 'w', encoding='utf-8') as f:
        for result in results:
            status = result['status']
            stats[status] += 1
            
            if status == 'success':
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                stats['total_patches'] += result['num_patches']
                
                # æ›´æ–°èŒƒå›´ç»Ÿè®¡
                for key, (min_val, max_val) in result['ranges'].items():
                    ranges_stats[key]['min'] = min(ranges_stats[key]['min'], min_val)
                    ranges_stats[key]['max'] = max(ranges_stats[key]['max'], max_val)
                
                # é€‰æ‹©prompt
                prompt = random.choice(PROMPTS) if use_diverse_prompts else PROMPTS[1]
                
                # è·å–npzæ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
                rel_npz_path = os.path.relpath(result['npz_path'], output_dir)
                
                # åˆ›å»ºLLaMA Factoryæ ¼å¼çš„å¯¹è¯æ•°æ®
                data = {
                    "messages": [
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": prompt},
                        {"role": "assistant", "content": result['label']}
                    ],
                    "images": [rel_npz_path]
                }
                
                # å†™å…¥jsonl
                f.write(json.dumps(data, ensure_ascii=False) + "\n")
    
    # æ‰“å°å¤„ç†ç»Ÿè®¡
    print(f"\nå¤„ç†å®Œæˆ! ç»“æœä¿å­˜åœ¨ {jsonl_path}")
    print(f"æ€»æ–‡ä»¶æ•°: {len(all_npy_files)}")
    print(f"æˆåŠŸå¤„ç†: {stats['success']}")
    print(f"å·²å­˜åœ¨è·³è¿‡: {stats['skipped']}")
    print(f"å¤„ç†å¤±è´¥: {stats['failed']}")
    print(f"å‘ç”Ÿé”™è¯¯: {stats['error']}")
    print(f"ç”Ÿæˆpatchæ€»æ•°: {stats['total_patches']}")
    
    if stats['success'] > 0:
        print(f"å¹³å‡æ¯ä¸ªç‚¹äº‘çš„patchæ•°: {stats['total_patches'] / stats['success']:.2f}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_file = os.path.join(output_dir, "objaverse_lvis_stats.json")
    with open(stats_file, "w") as f:
        json.dump({
            "stats": stats,
            "ranges": to_python_type(dict(ranges_stats))
        }, f, indent=2)
    
    print(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ° {stats_file}")
    
    return stats['success']

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Process Objaverse-LVIS point cloud data for LLaMA-Factory training')
    parser.add_argument('--root_dir', type=str, default="uni3d_data/objaverse_lvis", help='Root directory containing npy files')
    parser.add_argument('--output_dir', type=str, default="processed_objaverse", help='Output directory for processed data')
    parser.add_argument('--n_proc', type=int, default=None, help='Number of processes to use (default: CPU count - 1)')
    parser.add_argument('--diverse_prompts', action='store_true', help='Use diverse prompts for better generalization')
    parser.add_argument('--normalize', choices=['xyz', 'rgb', 'all', None], default='all', 
                      help='Normalize coordinates and/or colors (xyz: normalize coordinates, rgb: normalize colors, all: normalize both)')
    args = parser.parse_args()
    
    # å¤„ç†æ•°æ®
    process_objaverse_lvis(
        args.root_dir,
        args.output_dir,
        n_proc=args.n_proc,
        use_diverse_prompts=args.diverse_prompts,
        normalize=args.normalize
    )
    
    print("\nDone! ğŸ‰")
    print(f"To use with LLaMA-Factory, set --dataset_dir to {args.output_dir}")